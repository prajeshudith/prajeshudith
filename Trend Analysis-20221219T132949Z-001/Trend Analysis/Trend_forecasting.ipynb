{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"id":"O3UUFo345SRx","executionInfo":{"status":"error","timestamp":1663342214150,"user_tz":-330,"elapsed":7057,"user":{"displayName":"Prajesh Saravanan","userId":"05121487120956653596"}},"outputId":"c273c95c-e2af-4004-d208-9d7c270d62af"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-190c6c981fa9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_objects\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mplotnine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstreamlit\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_measures\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'streamlit'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["\n","#Importing the libraries\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy.stats import linregress\n","import pandas as pd\n","from datetime import datetime\n","import plotly.express as px\n","import seaborn as sns\n","import plotly.graph_objects as go\n","from plotnine import *\n","import streamlit as st\n","from statsmodels.tools.eval_measures import rmse\n","from sklearn.preprocessing import MinMaxScaler\n","from keras.preprocessing.sequence import TimeseriesGenerator\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.layers import Dropout\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","\n","##FOR SALES##\n","#Importing the dataset\n","df = pd.read_excel(r'/content/drive/MyDrive/Team Folder/Datasets/Book2_excel.xlsx')\n","\n","#Filtering only the required columns\n","df2 = df[[\"Section\",\"Quantity\",\"Transaction_Date\"]]\n","\n","#Sorting by date\n","df3 = df2.sort_values(by = \"Transaction_Date\")\n","\n","#Finding the total sales of ticket in a section per date\n","df4 = df3.groupby(['Section','Transaction_Date'],as_index =  False)['Quantity'].sum()\n","\n","#Combing then Clubs\n","club_1 = df4[df4.Section == \"Club Hall of Fame\"]\n","club_2 = df4[df4.Section == \"Club Main\"]\n","club_3 = df4[df4.Section == \"Club Mezzanine\"]\n","club_4 = [club_1,club_2,club_3]\n","club_5 = pd.concat(club_4)\n","\n","Club = club_5.groupby(['Transaction_Date'],as_index =  False)['Quantity'].sum()\n","\n","Club[\"Section\"] = \"Overall Club\"\n","Club = Club[[\"Section\",\"Transaction_Date\",\"Quantity\"]]\n","\n","#Combining the Floors\n","floor_1 = df4[df4.Section == \"FN5\"]\n","floor_2 = df4[df4.Section == \"Floor FE\"]\n","floor_3 = df4[df4.Section == \"Floor FN\"]\n","floor_4 = df4[df4.Section == \"Floor FS\"]\n","floor_5 = df4[df4.Section == \"Floor FW\"]\n","floor_6 = [floor_1,floor_2,floor_3,floor_4,floor_5]\n","floor_7 = pd.concat(floor_6)\n","\n","Floor = floor_7.groupby(['Transaction_Date'],as_index =  False)['Quantity'].sum()\n","\n","Floor[\"Section\"] = \"Overall Floor\"\n","Floor = Floor[[\"Section\",\"Transaction_Date\",\"Quantity\"]]\n","\n","#Combing the Halls\n","hall_1 = df4[df4.Section == \"Hall of Fame\"]\n","hall_2 = df4[df4.Section == \"Hall of Fame Suite\"]\n","hall_3 = [hall_1,hall_2]\n","hall_4 = pd.concat(hall_3)\n","\n","Hall = hall_4.groupby(['Transaction_Date'],as_index =  False)['Quantity'].sum()\n","\n","Hall[\"Section\"] = \"Overall Hall\"\n","Hall = Hall[[\"Section\",\"Transaction_Date\",\"Quantity\"]]\n","\n","#Appending with the Old dataframe\n","df5 =[df4,Club,Floor,Hall]\n","df6 = pd.concat(df5)\n","\n","#Heading\n","st.title(\"Trend Visualisation for sales of tickets\")\n","\n","#Selecting the section\n","section = st.sidebar.multiselect('Select a Sections',list(df6.Section.unique()))\n","\n","#Start date\n","start_date = st.sidebar.date_input('Select the Start date',value =pd.to_datetime(\"2021-03-23\", format=\"%Y-%m-%d\") ,min_value = pd.to_datetime(\"2021-03-23\", format=\"%Y-%m-%d\"),max_value=pd.to_datetime(\"2021-05-10\", format=\"%Y-%m-%d\"))\n","\n","#End date\n","end_date = st.sidebar.date_input('Select the End date',value =pd.to_datetime(\"2021-03-23\", format=\"%Y-%m-%d\") ,min_value = pd.to_datetime(\"2021-03-23\", format=\"%Y-%m-%d\"),max_value=pd.to_datetime(\"2021-05-10\", format=\"%Y-%m-%d\"))\n","\n","#Converting the date as string\n","start = start_date.strftime(\"%Y-%m-%d\")\n","end = end_date.strftime(\"%Y-%m-%d\")\n","\n","#Filtering with the input date\n","df8 = df6.loc[(df6[\"Transaction_Date\"]>= start)&(df6[\"Transaction_Date\"]<= end)]\n","\n","#Filtering with input sections\n","names = section\n","df9 = df8[df8.Section.isin(names)]\n","\n","#Selecting only dates and sales\n","df10 = df9[[\"Transaction_Date\",\"Quantity\"]]\n","\n","#Converting dates as datetime format\n","df10.Transaction_Date = pd.to_datetime(df10.Transaction_Date)\n","\n","#Converting date column as index\n","df11 = df10.set_index(\"Transaction_Date\")\n","\n","#Putting the dataframe into a variable \"train\"\n","train = df11\n","\n","#Model fitting for prediction\n","scaler = MinMaxScaler()\n","scaler.fit(train)\n","train = scaler.transform(train)\n","\n","n_input = 6\n","n_features = 1\n","generator = TimeseriesGenerator(train, train, length=n_input, batch_size=6)\n","\n","model = Sequential()\n","model.add(LSTM(200, activation='relu', input_shape=(n_input, n_features)))\n","model.add(Dropout(0.15))\n","model.add(Dense(1))\n","model.compile(optimizer='adam', loss='mse')\n","\n","model.fit_generator(generator,epochs=6)\n","\n","pred_list = []\n","\n","batch = train[-n_input:].reshape((1, n_input, n_features))\n","\n","for i in range(n_input):   \n","    pred_list.append(model.predict(batch)[0]) \n","    batch = np.append(batch[:,1:,:],[[pred_list[i]]],axis=1)\n","\n","from pandas.tseries.offsets import DateOffset\n","add_dates = [df11.index[-1] + DateOffset(days = x) for x in range(0,7) ]\n","future_dates = pd.DataFrame(index=add_dates[1:],columns=df11.columns)\n","\n","df_predict = pd.DataFrame(scaler.inverse_transform(pred_list),\n","                          index=future_dates[-n_input:].index, columns=['Prediction'])\n","\n","df_proj = pd.concat([df11,df_predict], axis=1)\n","\n","#Visualising the prediction graph\n","fig = px.scatter(df_proj,\n","    trendline_color_override='green'\n","                 \n",")\n","\n","fig.update_traces(mode = 'lines')\n","\n","#Editing date for acutual data including next 6 days\n","end_2 = end_date + DateOffset(days = 6)\n","\n","#Filtering the data according to date (+6 days from the end date selected for prediction)\n","df12 = df6.loc[(df6[\"Transaction_Date\"]>= start)&(df6[\"Transaction_Date\"]<= end_2)]\n","\n","#Filtering the sections\n","df13 = df12[df12.Section.isin(names)]\n","\n","#Plotting the acutal sales\n","fig2 = px.scatter(df13,x=\"Transaction_Date\",\n","    y=\"Quantity\",\n","    trendline=\"ols\",\n","    trendline_color_override='green',\n","    color=\"Section\"\n","                 \n",")\n","\n","fig2.update_traces(mode = 'lines')\n","\n","#Editing date for acutual data for excat dates\n","\n","#Filtering the data according to date (+6 days from the end date selected for prediction)\n","df21 = df6.loc[(df6[\"Transaction_Date\"]>= start)&(df6[\"Transaction_Date\"]<= end)]\n","\n","#Filtering the sections\n","df22 = df21[df21.Section.isin(names)]\n","\n","#Plotting the acutal sales\n","fig4 = px.scatter(df22,x=\"Transaction_Date\",\n","    y=\"Quantity\",\n","    trendline=\"ols\",\n","    trendline_color_override='green',\n","    color=\"Section\"\n","                 \n",")\n","\n","fig4.update_traces(mode = 'lines')\n","\n","##FOR PRICE##\n","\n","#Filtering only the required columns\n","df14 = df[[\"Section\",\"Display_Price_Per_Ticket_Amount\",\"Transaction_Date\"]]\n","\n","#Sorting by date\n","df15 = df14.sort_values(by = \"Transaction_Date\")\n","\n","#Finding the mean price of ticket in a section per date\n","df16 = df15.groupby(['Section','Transaction_Date'],as_index =  False)['Display_Price_Per_Ticket_Amount'].mean()\n","\n","#Combing then Clubs\n","club_6 = df16[df16.Section == \"Club Hall of Fame\"]\n","club_7 = df16[df16.Section == \"Club Main\"]\n","club_8 = df16[df16.Section == \"Club Mezzanine\"]\n","club_9 = [club_6,club_7,club_8]\n","club_10 = pd.concat(club_9)\n","\n","Club_11 = club_10.groupby(['Transaction_Date'],as_index =  False)['Display_Price_Per_Ticket_Amount'].mean()\n","\n","Club_11[\"Section\"] = \"Overall Club\"\n","Club_12 = Club_11[[\"Section\",\"Transaction_Date\",\"Display_Price_Per_Ticket_Amount\"]]\n","\n","#Combining the Floors\n","floor_8 = df16[df16.Section == \"FN5\"]\n","floor_9 = df16[df16.Section == \"Floor FE\"]\n","floor_10 = df16[df16.Section == \"Floor FN\"]\n","floor_11 = df16[df16.Section == \"Floor FS\"]\n","floor_12 = df16[df16.Section == \"Floor FW\"]\n","floor_13 = [floor_8,floor_9,floor_10,floor_11,floor_12]\n","floor_14 = pd.concat(floor_13)\n","\n","Floor_15 = floor_14.groupby(['Transaction_Date'],as_index =  False)['Display_Price_Per_Ticket_Amount'].mean()\n","\n","Floor_15[\"Section\"] = \"Overall Floor\"\n","Floor_16 = Floor_15[[\"Section\",\"Transaction_Date\",\"Display_Price_Per_Ticket_Amount\"]]\n","\n","#Combing the Halls\n","hall_5 = df16[df16.Section == \"Hall of Fame\"]\n","hall_6 = df16[df16.Section == \"Hall of Fame Suite\"]\n","hall_7 = [hall_5,hall_6]\n","hall_8 = pd.concat(hall_7)\n","\n","Hall_9 = hall_8.groupby(['Transaction_Date'],as_index =  False)['Display_Price_Per_Ticket_Amount'].mean()\n","\n","Hall_9[\"Section\"] = \"Overall Hall\"\n","Hall_10 = Hall_9[[\"Section\",\"Transaction_Date\",\"Display_Price_Per_Ticket_Amount\"]]\n","\n","#Appending with the Old dataframe\n","df17 =[df16,Club_12,Floor_16,Hall_10]\n","df18 = pd.concat(df17)\n","\n","#Filtering the data according to date (+6 days from the end date selected for prediction)\n","df19 = df18.loc[(df18[\"Transaction_Date\"]>= start)&(df18[\"Transaction_Date\"]<= end_2)]\n","\n","#Filtering the sections\n","df20 = df19[df19.Section.isin(names)]\n","\n","#Ploting the graph\n","fig3 = px.scatter(df20,\n","    x=\"Transaction_Date\",\n","    y=\"Display_Price_Per_Ticket_Amount\",\n","    trendline=\"ols\",\n","    trendline_color_override='green',\n","    color=\"Section\"\n","                 \n",")\n","\n","fig3.update_traces(mode = 'lines')\n","\n","#Plotting all the graphs\n","st.plotly_chart(fig4, use_container_width=True)\n","st.plotly_chart(fig, use_container_width=True)\n","st.plotly_chart(fig2, use_container_width=True)\n","st.plotly_chart(fig3, use_container_width=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dpDudTaXYmWE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663342207096,"user_tz":-330,"elapsed":25856,"user":{"displayName":"Prajesh Saravanan","userId":"05121487120956653596"}},"outputId":"e79c7177-2ebf-4820-ee38-4b0391d2684c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vPxKWJ1T3AWX","executionInfo":{"status":"ok","timestamp":1663342250904,"user_tz":-330,"elapsed":17155,"user":{"displayName":"Prajesh Saravanan","userId":"05121487120956653596"}},"outputId":"e2c88bcb-ad75-48cf-d26c-71a3c46bbb19"},"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: streamlit: command not found\n","\u001b[K\u001b[?25hnpx: installed 22 in 3.92s\n","your url is: https://fifty-donkeys-stick-35-237-104-141.loca.lt\n","^C\n"]}],"source":["!streamlit run app.py & npx localtunnel --port 8501"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KKNupfIh8UIa"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}