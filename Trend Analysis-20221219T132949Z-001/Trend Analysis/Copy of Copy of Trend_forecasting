{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"I8L9foPUQe9u"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O3UUFo345SRx"},"outputs":[],"source":["%%writefile app.py\n","#Importing the libraries\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy.stats import linregress\n","import pandas as pd\n","from datetime import datetime\n","import plotly.express as px\n","import seaborn as sns\n","import plotly.graph_objects as go\n","from plotnine import *\n","import streamlit as st\n","from statsmodels.tools.eval_measures import rmse\n","from sklearn.preprocessing import MinMaxScaler\n","from keras.preprocessing.sequence import TimeseriesGenerator\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.layers import Dropout\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","\n","##FOR SALES##\n","#Importing the dataset\n","df = pd.read_excel(r'/content/drive/MyDrive/Team Folder/Datasets/Book2_excel.xlsx')\n","\n","#Filtering only the required columns\n","df2 = df[[\"Section\",\"Quantity\",\"Transaction_Date\"]]\n","\n","#Sorting by date\n","df3 = df2.sort_values(by = \"Transaction_Date\")\n","\n","#Finding the total sales of ticket in a section per date\n","df4 = df3.groupby(['Section','Transaction_Date'],as_index =  False)['Quantity'].sum()\n","\n","#Combing then Clubs\n","club_1 = df4[df4.Section == \"Club Hall of Fame\"]\n","club_2 = df4[df4.Section == \"Club Main\"]\n","club_3 = df4[df4.Section == \"Club Mezzanine\"]\n","club_4 = [club_1,club_2,club_3]\n","club_5 = pd.concat(club_4)\n","\n","Club = club_5.groupby(['Transaction_Date'],as_index =  False)['Quantity'].sum()\n","\n","Club[\"Section\"] = \"Overall Club\"\n","Club = Club[[\"Section\",\"Transaction_Date\",\"Quantity\"]]\n","\n","#Combining the Floors\n","floor_1 = df4[df4.Section == \"FN5\"]\n","floor_2 = df4[df4.Section == \"Floor FE\"]\n","floor_3 = df4[df4.Section == \"Floor FN\"]\n","floor_4 = df4[df4.Section == \"Floor FS\"]\n","floor_5 = df4[df4.Section == \"Floor FW\"]\n","floor_6 = [floor_1,floor_2,floor_3,floor_4,floor_5]\n","floor_7 = pd.concat(floor_6)\n","\n","Floor = floor_7.groupby(['Transaction_Date'],as_index =  False)['Quantity'].sum()\n","\n","Floor[\"Section\"] = \"Overall Floor\"\n","Floor = Floor[[\"Section\",\"Transaction_Date\",\"Quantity\"]]\n","\n","#Combing the Halls\n","hall_1 = df4[df4.Section == \"Hall of Fame\"]\n","hall_2 = df4[df4.Section == \"Hall of Fame Suite\"]\n","hall_3 = [hall_1,hall_2]\n","hall_4 = pd.concat(hall_3)\n","\n","Hall = hall_4.groupby(['Transaction_Date'],as_index =  False)['Quantity'].sum()\n","\n","Hall[\"Section\"] = \"Overall Hall\"\n","Hall = Hall[[\"Section\",\"Transaction_Date\",\"Quantity\"]]\n","\n","#Appending with the Old dataframe\n","df5 =[df4,Club,Floor,Hall]\n","df6 = pd.concat(df5)\n","\n","#Heading\n","st.title(\"Trend Visualisation for sales of tickets\")\n","\n","#Selecting the section\n","section = st.sidebar.multiselect('Select a Sections',list(df6.Section.unique()))\n","\n","#Start date\n","start_date = st.sidebar.date_input('Select the Start date',value =pd.to_datetime(\"2021-03-23\", format=\"%Y-%m-%d\") ,min_value = pd.to_datetime(\"2021-03-23\", format=\"%Y-%m-%d\"),max_value=pd.to_datetime(\"2021-05-10\", format=\"%Y-%m-%d\"))\n","\n","#End date\n","end_date = st.sidebar.date_input('Select the End date',value =pd.to_datetime(\"2021-03-23\", format=\"%Y-%m-%d\") ,min_value = pd.to_datetime(\"2021-03-23\", format=\"%Y-%m-%d\"),max_value=pd.to_datetime(\"2021-05-10\", format=\"%Y-%m-%d\"))\n","\n","#Converting the date as string\n","start = start_date.strftime(\"%Y-%m-%d\")\n","end = end_date.strftime(\"%Y-%m-%d\")\n","\n","#Filtering with the input date\n","df8 = df6.loc[(df6[\"Transaction_Date\"]>= start)&(df6[\"Transaction_Date\"]<= end)]\n","\n","#Filtering with input sections\n","names = section\n","df9 = df8[df8.Section.isin(names)]\n","\n","#Selecting only dates and sales\n","df10 = df9[[\"Transaction_Date\",\"Quantity\"]]\n","\n","#Converting dates as datetime format\n","df10.Transaction_Date = pd.to_datetime(df10.Transaction_Date)\n","\n","#Converting date column as index\n","df11 = df10.set_index(\"Transaction_Date\")\n","\n","#Putting the dataframe into a variable \"train\"\n","train = df11\n","\n","#Model fitting for prediction\n","scaler = MinMaxScaler()\n","scaler.fit(train)\n","train = scaler.transform(train)\n","\n","n_input = 6\n","n_features = 1\n","generator = TimeseriesGenerator(train, train, length=n_input, batch_size=6)\n","\n","model = Sequential()\n","model.add(LSTM(200, activation='relu', input_shape=(n_input, n_features)))\n","model.add(Dropout(0.15))\n","model.add(Dense(1))\n","model.compile(optimizer='adam', loss='mse')\n","\n","model.fit_generator(generator,epochs=6)\n","\n","pred_list = []\n","\n","batch = train[-n_input:].reshape((1, n_input, n_features))\n","\n","for i in range(n_input):   \n","    pred_list.append(model.predict(batch)[0]) \n","    batch = np.append(batch[:,1:,:],[[pred_list[i]]],axis=1)\n","\n","from pandas.tseries.offsets import DateOffset\n","add_dates = [df11.index[-1] + DateOffset(days = x) for x in range(0,7) ]\n","future_dates = pd.DataFrame(index=add_dates[1:],columns=df11.columns)\n","\n","df_predict = pd.DataFrame(scaler.inverse_transform(pred_list),\n","                          index=future_dates[-n_input:].index, columns=['Prediction'])\n","\n","df_proj = pd.concat([df11,df_predict], axis=1)\n","\n","#Visualising the prediction graph\n","fig = px.scatter(df_proj,\n","    trendline_color_override='green'\n","                 \n",")\n","\n","fig.update_traces(mode = 'lines')\n","\n","#Editing date for acutual data including next 6 days\n","end_2 = end_date + DateOffset(days = 6)\n","\n","#Filtering the data according to date (+6 days from the end date selected for prediction)\n","df12 = df6.loc[(df6[\"Transaction_Date\"]>= start)&(df6[\"Transaction_Date\"]<= end_2)]\n","\n","#Filtering the sections\n","df13 = df12[df12.Section.isin(names)]\n","\n","#Plotting the acutal sales\n","fig2 = px.scatter(df13,x=\"Transaction_Date\",\n","    y=\"Quantity\",\n","    trendline=\"ols\",\n","    trendline_color_override='green',\n","    color=\"Section\"\n","                 \n",")\n","\n","fig2.update_traces(mode = 'lines')\n","\n","#Editing date for acutual data for excat dates\n","\n","#Filtering the data according to date (+6 days from the end date selected for prediction)\n","df21 = df6.loc[(df6[\"Transaction_Date\"]>= start)&(df6[\"Transaction_Date\"]<= end)]\n","\n","#Filtering the sections\n","df22 = df21[df21.Section.isin(names)]\n","\n","#Plotting the acutal sales\n","fig4 = px.scatter(df22,x=\"Transaction_Date\",\n","    y=\"Quantity\",\n","    trendline=\"ols\",\n","    trendline_color_override='green',\n","    color=\"Section\"\n","                 \n",")\n","\n","fig4.update_traces(mode = 'lines')\n","\n","##FOR PRICE##\n","\n","#Filtering only the required columns\n","df14 = df[[\"Section\",\"Display_Price_Per_Ticket_Amount\",\"Transaction_Date\"]]\n","\n","#Sorting by date\n","df15 = df14.sort_values(by = \"Transaction_Date\")\n","\n","#Finding the mean price of ticket in a section per date\n","df16 = df15.groupby(['Section','Transaction_Date'],as_index =  False)['Display_Price_Per_Ticket_Amount'].mean()\n","\n","#Combing then Clubs\n","club_6 = df16[df16.Section == \"Club Hall of Fame\"]\n","club_7 = df16[df16.Section == \"Club Main\"]\n","club_8 = df16[df16.Section == \"Club Mezzanine\"]\n","club_9 = [club_6,club_7,club_8]\n","club_10 = pd.concat(club_9)\n","\n","Club_11 = club_10.groupby(['Transaction_Date'],as_index =  False)['Display_Price_Per_Ticket_Amount'].mean()\n","\n","Club_11[\"Section\"] = \"Overall Club\"\n","Club_12 = Club_11[[\"Section\",\"Transaction_Date\",\"Display_Price_Per_Ticket_Amount\"]]\n","\n","#Combining the Floors\n","floor_8 = df16[df16.Section == \"FN5\"]\n","floor_9 = df16[df16.Section == \"Floor FE\"]\n","floor_10 = df16[df16.Section == \"Floor FN\"]\n","floor_11 = df16[df16.Section == \"Floor FS\"]\n","floor_12 = df16[df16.Section == \"Floor FW\"]\n","floor_13 = [floor_8,floor_9,floor_10,floor_11,floor_12]\n","floor_14 = pd.concat(floor_13)\n","\n","Floor_15 = floor_14.groupby(['Transaction_Date'],as_index =  False)['Display_Price_Per_Ticket_Amount'].mean()\n","\n","Floor_15[\"Section\"] = \"Overall Floor\"\n","Floor_16 = Floor_15[[\"Section\",\"Transaction_Date\",\"Display_Price_Per_Ticket_Amount\"]]\n","\n","#Combing the Halls\n","hall_5 = df16[df16.Section == \"Hall of Fame\"]\n","hall_6 = df16[df16.Section == \"Hall of Fame Suite\"]\n","hall_7 = [hall_5,hall_6]\n","hall_8 = pd.concat(hall_7)\n","\n","Hall_9 = hall_8.groupby(['Transaction_Date'],as_index =  False)['Display_Price_Per_Ticket_Amount'].mean()\n","\n","Hall_9[\"Section\"] = \"Overall Hall\"\n","Hall_10 = Hall_9[[\"Section\",\"Transaction_Date\",\"Display_Price_Per_Ticket_Amount\"]]\n","\n","#Appending with the Old dataframe\n","df17 =[df16,Club_12,Floor_16,Hall_10]\n","df18 = pd.concat(df17)\n","\n","#Filtering the data according to date (+6 days from the end date selected for prediction)\n","df19 = df18.loc[(df18[\"Transaction_Date\"]>= start)&(df18[\"Transaction_Date\"]<= end_2)]\n","\n","#Filtering the sections\n","df20 = df19[df19.Section.isin(names)]\n","\n","#Ploting the graph\n","fig3 = px.scatter(df20,\n","    x=\"Transaction_Date\",\n","    y=\"Display_Price_Per_Ticket_Amount\",\n","    trendline=\"ols\",\n","    trendline_color_override='green',\n","    color=\"Section\"\n","                 \n",")\n","\n","fig3.update_traces(mode = 'lines')\n","\n","#Plotting all the graphs\n","st.plotly_chart(fig4, use_container_width=True)\n","st.plotly_chart(fig, use_container_width=True)\n","st.plotly_chart(fig2, use_container_width=True)\n","st.plotly_chart(fig3, use_container_width=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DfnfsCvFsl5V"},"outputs":[],"source":["pip install streamlit"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"vPxKWJ1T3AWX","outputId":"8eca7648-cdb3-4a17-f33b-183e99581f6c"},"outputs":[{"name":"stdout","output_type":"stream","text":["2022-05-18 13:03:03.334 INFO    numexpr.utils: NumExpr defaulting to 2 threads.\n","\u001b[0m\n","\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n","\u001b[0m\n","\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.2:8501\u001b[0m\n","\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.196.203.202:8501\u001b[0m\n","\u001b[0m\n","\u001b[K\u001b[?25hnpx: installed 22 in 5.281s\n","your url is: https://sixty-rooms-eat-35-196-203-202.loca.lt\n","2022-05-18 13:41:25.658 Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/streamlit/scriptrunner/script_runner.py\", line 475, in _run_script\n","    exec(code, module.__dict__)\n","  File \"/content/app.py\", line 114, in <module>\n","    scaler.fit(train)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py\", line 416, in fit\n","    return self.partial_fit(X, y)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py\", line 458, in partial_fit\n","    force_all_finite=\"allow-nan\",\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 566, in _validate_data\n","    X = check_array(X, **check_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 808, in check_array\n","    % (n_samples, array.shape, ensure_min_samples, context)\n","ValueError: Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by MinMaxScaler.\n","\n","2022-05-18 13:42:00.685 Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/streamlit/scriptrunner/script_runner.py\", line 475, in _run_script\n","    exec(code, module.__dict__)\n","  File \"/content/app.py\", line 119, in <module>\n","    generator = TimeseriesGenerator(train, train, length=n_input, batch_size=6)\n","  File \"/usr/local/lib/python3.7/dist-packages/keras_preprocessing/sequence.py\", line 357, in __init__\n","    % (self.start_index, self.end_index))\n","ValueError: `start_index+length=6 > end_index=0` is disallowed, as no part of the sequence would be left to be used as current step.\n","\n","2022-05-18 13:42:03.062 Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/streamlit/scriptrunner/script_runner.py\", line 475, in _run_script\n","    exec(code, module.__dict__)\n","  File \"/content/app.py\", line 119, in <module>\n","    generator = TimeseriesGenerator(train, train, length=n_input, batch_size=6)\n","  File \"/usr/local/lib/python3.7/dist-packages/keras_preprocessing/sequence.py\", line 357, in __init__\n","    % (self.start_index, self.end_index))\n","ValueError: `start_index+length=6 > end_index=0` is disallowed, as no part of the sequence would be left to be used as current step.\n","\n","2022-05-18 13:42:10.167131: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","Epoch 1/6\n","1/1 [==============================] - 2s 2s/step - loss: 0.0417\n","Epoch 2/6\n","1/1 [==============================] - 0s 19ms/step - loss: 0.0341\n","Epoch 3/6\n","1/1 [==============================] - 0s 19ms/step - loss: 0.0294\n","Epoch 4/6\n","1/1 [==============================] - 0s 17ms/step - loss: 0.0207\n","Epoch 5/6\n","1/1 [==============================] - 0s 21ms/step - loss: 0.0163\n","Epoch 6/6\n","1/1 [==============================] - 0s 18ms/step - loss: 0.0122\n"]}],"source":["!streamlit run app.py & npx localtunnel --port 8501"]},{"cell_type":"markdown","metadata":{"id":"LUde3JxhsCKd"},"source":["*italicized text*# New Section"]},{"cell_type":"markdown","metadata":{"id":"TJQwWHp-sCu-"},"source":["# New Section"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Copy of Copy of Trend_forecasting","provenance":[{"file_id":"1-AqMSmgLz03D-PnC6vSVhKX5cjRm2ANR","timestamp":1651669958129}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}